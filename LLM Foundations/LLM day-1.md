# LLM Foundations

## Traditional Programming (Algorithmic)

- In Traditional Programming, Everything should be done by the developer / programmer.

## Machine Learning (Driven)

- In Machine Learning, The developer trains a machine with huge amount of data then the machine will control all the things.

## Types of Machine Learning

- Supervised Learning - where model trains from labeled data
- Unsupervised Learning - where model trains from unlabeled data
- Reinforcement Learning - where model trains based on the rewards

---

# Deep Learning

- There are many methods in machine learning but now-a-days the dominant one is Neural Network (NN) also known as Deep Learning (DL)

## Neural Networks (NN)

- The taught of neural network is to work like a human brain
- Like how a brain consists of billions of neurons, the neural network also consists neurons for computing

![ **Human Neuron vs Perceptron**](LLM%20Foundations%2073163815a02e409d8d14d0894a7d2c8c/Untitled.png)

 **Human Neuron vs Perceptron**

- This is how a artificial neurons came from the neurons of human brain
- All NN operations are just matrix multiplications and for those multiplications GPUs are really fast

### Training a Neural Network

- Data X ( input data ) and Labels Y ( output data )
- Take a small set from X and Y as x and y:
    
    → use current model to make prediction yp
    
    → compute loss(y,yp)
    
    → Back Propagate the loss through all the layers of the model
    
- Repeat until loss stops decreasing

### Pre training vs Fine tuning

- Pre training means the LLM  is trained with a huge amount of data whereas, fine tuning is training the LLM with small amount of data

---

## Transformers

- Before 2020, every task had it’s own NN architecture but now all is transformers

![**Transformers**](LLM%20Foundations%2073163815a02e409d8d14d0894a7d2c8c/Untitled%201.png)

**Transformers**

### Inputs

- Machines can evaluate only numerical data
- From the given original input (text), split into sequence of tokens
- replace the tokens with the vocabulary ID of that token
- Each ID can be represented by a one hot vector generated by one hot encoder.

### Input Embedding

- One hot encoding are poor representations of sequence of tokens because it doesn’t check for the similarity. Even the similar words are treated as different words
- to solve this, we embed the input by embedding matrix
- this step vastly reduce the input size

### Masked Multi-Head Attention

**Attention**

- Attention finds the most important words in the vocabulary similar to human brain that makes attention

**Self Attention**

- Self attention takes a input as sequence of inputs and generates a output as sequence of vectors each one is weighted sum of the sequence
- This mechanism makes use of three main components namely the queries **Q**, the keys **K** and the values **V**

**Multi-head attention**

- It is similar to self attention but the difference is, It allows different ways of transforming into queries, keys  and values to be learned
- It happens in several different ways simultaneously

**Masking Attention**

- In masking attention, all probabilities are computed at the same time
- mask attention is limited to just the part of the input that’s already seen

### Positional Encoding

- Generally the attention is totally position-invariant
- To fix this, we use Positional encoding to embedded vectors

### Add

- It allows gradient to flow from the loss function all the to the first layer

### Norm

- Neural network modules perform best when input vectors  have uniform mean and SD in each dimension
- As inputs flow through the network, means and SD are blown out
- layer normalization is used to reset the things to where we want them in b/w layers

### Feed Forward Layer

- It is a standard multi perceptron with one hidden layer
- the tokens comes into feed forward layer and upgrades it’s representation

## Transformer Architecture

- The main transformer layer is stacked many times
- overall hyperparameters are
    
    → number of layers
    
    → embedding dimension
    
    → number of attention heads
    

## Transformers used in LLMs

- BERT (Bidirectional Encoder Representations from Transformers)
- T5  (Text-to-Text Transfer Transformer)
- GPT (Generative Pre-trained Transformer)